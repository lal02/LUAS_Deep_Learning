{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "\n",
    "import tensorflow as tf \n",
    "import keras \n",
    "from keras import layers\n",
    "df = pd.read_csv(\"../datasets/Task_1/7.csv\")\n",
    "df.head()\n",
    "X = df.drop([\"Price\",\"ID\"], axis=1)\n",
    "y = df[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr(numeric_only=True)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this doesnt finish for some reason, just runs forever\n",
    "\n",
    "#from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "# get the fisher's score rankings \n",
    "#ranks = fisher_score.fisher_score(X.values, y.values)\n",
    "\n",
    "# create a pandas DataFrame for easier interpretation\n",
    "#feat_importances = pd.Series(ranks, X.columns)\n",
    "#feat_importances.plot(kind='barh')\n",
    "\n",
    "# how to interpret -> low score means the effect of this field is not large in the dataset\n",
    "# => typically means other columns in the dataset have similar correlations, \n",
    "# therefore making this particular column not so useful since other columns \n",
    "# already fill this role for this correlation\n",
    "\n",
    "# Fisher's score studies the variance of the data -> statistical significance'\n",
    "\n",
    "# based on Fisher's score:\n",
    "# num_rooms is not important at all, and num_people has minor importance in this data\n",
    "# but ave_monthly_income is quite powerful based on Fisher's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# convert all continuous variables to integer,\n",
    "# and convert all negative numbers to 0\n",
    "X_cat = X.astype(int)\n",
    "X_cat = X_cat.clip(lower=0)\n",
    "\n",
    "# initialize chi2 and SelectKBest\n",
    "# Note: chi2 -test is a very common test\n",
    "# in statistics and quantitative analysis\n",
    "# basically it studies the data whether variables are related\n",
    "# or independent of each other\n",
    "chi_2_features = SelectKBest(chi2, k=len(X_cat.columns))\n",
    "\n",
    "# fit our data to the SelectKBest\n",
    "best_features = chi_2_features.fit(X_cat,y.astype(int))\n",
    "\n",
    "# use decimal format in table print later\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# wrap it up, and show the results\n",
    "# the higher the score, the more effect that column has on target variable\n",
    "df_features = pd.DataFrame(best_features.scores_)\n",
    "df_columns = pd.DataFrame(X_cat.columns)\n",
    "f_scores = pd.concat([df_columns,df_features],axis=1)\n",
    "f_scores.columns = ['Features','Score']\n",
    "f_scores.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# step 2, split the temporary data in HALF (0.5) => 15% test and 15% validation\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# save the amount of support variables into a helper variable\n",
    "# so we don't have to update the input_shape all the time\n",
    "variable_amount = len(X.columns)\n",
    "\n",
    "# create callbacks and place them into a parameter list\n",
    "# NOTE! if you get PermissionError while training the model,\n",
    "# just try training it again\n",
    "mc = ModelCheckpoint('best_model_regression1_cars.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# if you use multiple callbacks (EarlyStoppin, ReduceLROnPlateau etc.)\n",
    "# add them to this same list\n",
    "callback_list = [mc]\n",
    "\n",
    "# let's try some common optimization approaches\n",
    "\n",
    "# neural networks often need at least a normalization layer\n",
    "# so that it updates all weight values fairly \n",
    "# typically the original dataset has various scales of numbers\n",
    "# which confuses neural network while it's training itself\n",
    "# luckily we have the BatchNormalization -layer in keras!\n",
    "\n",
    "# regularization is often beneficial in neural networks\n",
    "# but it's usually better to apply this a bit later\n",
    "# once you know approximately a working neural network structure for your data\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(variable_amount,)),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select the optimizer and loss function\n",
    "# you can try rmsprop also as optimizer, or stochastic gradient descent\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# common tips on how to change neural network structure if your metrics are not good:\n",
    "#####################################################################################\n",
    "# make wider (or narrower) layers (for example, 64 or 128 nodes)\n",
    "# make a longer or shorter network (add or remove layers)\n",
    "# use Dropout -layers (e.g. layers.Dropout(0.1))\n",
    "\n",
    "# remember: there's no process or mathematical formula\n",
    "# in order to figure out the optimal neural network structure\n",
    "# it's mostly all about trial and error => EXPERIMENTATION!\n",
    "\n",
    "# remember to have enough \"decision-space\" for your data!\n",
    "# it's highly unlikely a dataset with 20 different variables is going\n",
    "# to work well with only 8 nodes in each layer etc.\n",
    "\n",
    "# print out the summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=1200, validation_data=(X_val, y_val), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd534627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use pandas for this (easy code)\n",
    "# try to look if the model is actually training \n",
    "# => the error is going downwards\n",
    "# if using validation data, you get two lines\n",
    "# in this case, see if the lines follow a similar trend \n",
    "# (they don't always overlap with complex data, the trend is more important)\n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39783c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"best_model_regression1_cars.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the final model loss/evaluation values\n",
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ec4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# reshape the data for easier comparison table\n",
    "test_predictions = pd.Series(test_predictions.reshape(len(y_test),))\n",
    "pred_df = pd.DataFrame(np.asarray(y_test), columns=['Test True Y'])\n",
    "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
    "pred_df.columns = ['Test True Y', 'Model Predictions']\n",
    "\n",
    "# print the comparison table - true values vs. model predicted values\n",
    "# we can nicely see here how far off our model is in some cases\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77874b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Mean average error\n",
    "print(\"MAE\")\n",
    "print(round(metrics.mean_absolute_error(y_test, test_predictions), 2), \"$\")\n",
    "\n",
    "# MSE - Mean square error\n",
    "print(\"\\nMSE\")\n",
    "print(round(metrics.mean_squared_error(y_test, test_predictions), 2), \"$^2\")\n",
    "\n",
    "# RMSE - Root mean square error\n",
    "print('\\nRMSE:')\n",
    "print(round(np.sqrt(metrics.mean_squared_error(y_test, test_predictions)), 2), \"$\")\n",
    "\n",
    "# R-squared. 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "print('\\nR-squared:')\n",
    "print(round(metrics.r2_score(y_test, test_predictions), 2))\n",
    "\n",
    "# Explained Variance Score => 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "# high variance score = model is a good fit for the data \n",
    "# low variance score = model is not a good fit for the data\n",
    "# the higher the score, the model is more able to explain the variation in the data\n",
    "# if score is low, we might need more and better data\n",
    "print(\"\\nExplained variance score:\")\n",
    "print(round(metrics.explained_variance_score(y_test, test_predictions), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the prediction distribution are far from normal distribution\n",
    "# then the model is not probably good enough\n",
    "# distplot is deprecating in future pandas-version\n",
    "# unfortunately, there's no exact alternative to do this plot at the moment\n",
    "sns.distplot((y_test - test_predictions))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
