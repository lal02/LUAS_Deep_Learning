{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(500):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for the target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        # the only point of this is to have some kind of logic in the data\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m original_b3 = bias3\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# use generated training data from our helper function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m data = \u001b[43mgenerate_train_data\u001b[49m()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# learning rate\u001b[39;00m\n\u001b[32m     34\u001b[39m LR = \u001b[32m0.0125\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "\n",
    "\n",
    "#w1 nd w5very high starting points\n",
    "\n",
    "w1 = 0.3\n",
    "w2 = 0.5\n",
    "w3 = 0.3\n",
    "w4 = -0.5\n",
    "w5 = 1.5\n",
    "w6 = 1.2\n",
    "bias1 = 0.5\n",
    "bias2 = -0.35\n",
    "bias3 = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# use generated training data from our helper function\n",
    "data = generate_train_data()\n",
    "\n",
    "# learning rate\n",
    "LR = 0.0125\n",
    "epochs = 5000\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 0.3\n",
      "w2: 0.5\n",
      "w3: 0.3\n",
      "w4: -0.5\n",
      "w5: 1.5\n",
      "w6: 1.2\n",
      "b1: 0.5\n",
      "b2: -0.35\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 2.125308082469445\n",
      "w2: 2.7261499911510754\n",
      "w3: -9.986796084337133\n",
      "w4: -11.492272659940818\n",
      "w5: -3.206217819721507\n",
      "w6: -3.1569782952115837\n",
      "b1: -1.032083037642287\n",
      "b2: -1.9786624616537523\n",
      "b3: 12.619863706019384\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 9]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.619863706019384"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(1, 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
